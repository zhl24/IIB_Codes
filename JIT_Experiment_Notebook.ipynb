{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from JIT_accelerated_code import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[2], line 58\u001b[0m\n\u001b[0;32m     56\u001b[0m langevin \u001b[38;5;241m=\u001b[39m SDE(A,h,T,normal_gamma_generator)\n\u001b[0;32m     57\u001b[0m \u001b[38;5;66;03m#Noisy observation generation\u001b[39;00m\n\u001b[1;32m---> 58\u001b[0m SDE_samples,system_jumps,NVM_jumps,subordinator_jumps,jump_times \u001b[38;5;241m=\u001b[39m \u001b[43mlangevin\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgenerate_samples\u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_points\u001b[49m\u001b[43m,\u001b[49m\u001b[43mall_data\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     59\u001b[0m d1,d2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mshape(SDE_samples)\n\u001b[0;32m     60\u001b[0m \u001b[38;5;66;03m#Noisy_samples = SDE_samples + np.random.randn(d1,d2)*sigma_n #The noisy observations simulated. Already in the column vector form\u001b[39;00m\n\u001b[0;32m     61\u001b[0m \n\u001b[0;32m     62\u001b[0m \n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m#Create the partial observation, observing only the integral state x here.\u001b[39;00m\n",
      "File \u001b[1;32me:\\Cambridge\\Year 4\\Fourth Year Projcet\\IIB_Codes\\NVM_state_space.py:59\u001b[0m, in \u001b[0;36mSDE.generate_samples\u001b[1;34m(self, evaluation_points, plot_NVM, plot_jumps, all_data)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m j,jump_time \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(jump_times):\n\u001b[0;32m     58\u001b[0m     NVM_jump \u001b[38;5;241m=\u001b[39m NVM_jumps[j]\n\u001b[1;32m---> 59\u001b[0m     system_jump \u001b[38;5;241m=\u001b[39m NVM_jump \u001b[38;5;241m*\u001b[39m \u001b[43mexpm\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mA\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m \u001b[49m\u001b[43m(\u001b[49m\u001b[43mevaluation_point\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43mjump_time\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;241m@\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mh\n\u001b[0;32m     60\u001b[0m     system_jumps\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mlist\u001b[39m(system_jump))\n\u001b[0;32m     62\u001b[0m samples\u001b[38;5;241m.\u001b[39mappend(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgeneral_integrate([evaluation_point],system_jumps,jump_times)[\u001b[38;5;241m0\u001b[39m])\u001b[38;5;66;03m#Extracting the single element at evaluation\u001b[39;00m\n",
      "File \u001b[1;32me:\\Cambridge\\Year 4\\Fourth Year Projcet\\source_code\\y4proj_env\\Lib\\site-packages\\scipy\\linalg\\_matfuncs.py:309\u001b[0m, in \u001b[0;36mexpm\u001b[1;34m(A)\u001b[0m\n\u001b[0;32m    303\u001b[0m \u001b[38;5;66;03m# Explicit formula for 2x2 case, formula (2.2) in [1]\u001b[39;00m\n\u001b[0;32m    304\u001b[0m \u001b[38;5;66;03m# without Kahan's method numerical instabilities can occur.\u001b[39;00m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m a\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m:] \u001b[38;5;241m==\u001b[39m (\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m2\u001b[39m):\n\u001b[0;32m    306\u001b[0m     a1, a2, a3, a4 \u001b[38;5;241m=\u001b[39m (a[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, [\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m0\u001b[39m]],\n\u001b[0;32m    307\u001b[0m                       a[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, [\u001b[38;5;241m0\u001b[39m], [\u001b[38;5;241m1\u001b[39m]],\n\u001b[0;32m    308\u001b[0m                       a[\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;241m.\u001b[39m, [\u001b[38;5;241m1\u001b[39m], [\u001b[38;5;241m0\u001b[39m]],\n\u001b[1;32m--> 309\u001b[0m                       \u001b[43ma\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m]\u001b[49m)\n\u001b[0;32m    310\u001b[0m     mu \u001b[38;5;241m=\u001b[39m csqrt((a1\u001b[38;5;241m-\u001b[39ma4)\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m2\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m4\u001b[39m\u001b[38;5;241m*\u001b[39ma2\u001b[38;5;241m*\u001b[39ma3)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.\u001b[39m  \u001b[38;5;66;03m# csqrt slow but handles neg.vals\u001b[39;00m\n\u001b[0;32m    312\u001b[0m     eApD2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mexp((a1\u001b[38;5;241m+\u001b[39ma4)\u001b[38;5;241m/\u001b[39m\u001b[38;5;241m2.\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy.linalg import expm #This is the automatic matrix expnent solver\n",
    "from jump_process_generators import *\n",
    "import matplotlib.pyplot as plt\n",
    "from NVM_state_space import *\n",
    "from NVM_state_space_filters import *\n",
    "from basic_tools import *\n",
    "from scipy.special import logsumexp\n",
    "from tqdm import tqdm  # 导入tqdm\n",
    "import seaborn as sns\n",
    "#Again, we first generate the noisy observations.\n",
    "#We again have the noisy data first:\n",
    "#Again, we first generate the noisy observations.\n",
    "#We again have the noisy data first:\n",
    "\n",
    "\n",
    "#beta_pcn = 1 #The step parameter for the pre-conditioned crank nicolson algorithm\n",
    "n_iter = 10 #Number of iterations\n",
    "theta0 = -2 #Initial guess of theta\n",
    "num_particles = 10\n",
    "l = 1 #Step size\n",
    "K = l**2 # Squared length of the Gaussian \"kernel\". Since in this case our space is 1D, K is just the Gaussian variance.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "true_theta = -1 #The true theta value for the Langevin system\n",
    "\n",
    "\n",
    "kw = 1 #The prior parameter for muw\n",
    "\n",
    "kv = 0.001 #The observation noise scaling factor. Note that this scaling factor applies to the variance\n",
    "true_sigmaw = 2\n",
    "true_muw = 0.2\n",
    "sigma_n = true_sigmaw * np.sqrt(kv)\n",
    "\n",
    "\n",
    "#Simulation Parameters\n",
    "\n",
    "beta = 5\n",
    "C = 10\n",
    "T = 10\n",
    "\n",
    "\n",
    "N = 10*T  # Resolution\n",
    "\n",
    "#Define the Langevin dynamics\n",
    "A = np.zeros((2, 2))\n",
    "A[0, 1] = 1\n",
    "A[1, 1] = true_theta\n",
    "h = np.array([[0], [1]])\n",
    "\n",
    "#Simulation\n",
    "evaluation_points = np.linspace(0, T, N) #Note that this would be the time axis we work on.\n",
    "normal_gamma_generator = normal_gamma_process(beta, C, T, true_muw, true_sigmaw)\n",
    "langevin = SDE(A,h,T,normal_gamma_generator)\n",
    "#Noisy observation generation\n",
    "SDE_samples,system_jumps,NVM_jumps,subordinator_jumps,jump_times = langevin.generate_samples(evaluation_points,all_data=True)\n",
    "d1,d2 = np.shape(SDE_samples)\n",
    "#Noisy_samples = SDE_samples + np.random.randn(d1,d2)*sigma_n #The noisy observations simulated. Already in the column vector form\n",
    "\n",
    "\n",
    "#Create the partial observation, observing only the integral state x here.\n",
    "Noisy_samples = SDE_samples[:,0] + np.random.randn(np.shape(SDE_samples)[0])*sigma_n #The noisy observations simulated. Already in the column vector form\n",
    "\n",
    "theta_samples = [theta0]\n",
    "first_time = True\n",
    "\n",
    "for iter in tqdm(range(n_iter), desc=\"Processing\"):\n",
    "\n",
    "\n",
    "\n",
    "    if first_time:\n",
    "        #print(\"Progress:\",progress/searching_resolution, \"%\")\n",
    "        #progress +=1\n",
    "        #Prior inverted gamma parameters for sigmaw\n",
    "        alphaws = 2.1 * np.ones(num_particles)\n",
    "        betaws = 1 * np.ones(num_particles)\n",
    "        accumulated_Es = np.zeros(num_particles)\n",
    "        accumulated_Fs = np.zeros(num_particles)\n",
    "\n",
    "        trajectory = []\n",
    "        A = np.zeros((2, 2))\n",
    "        A[0, 1] = 1\n",
    "        A[1, 1] = theta_samples[-1]\n",
    "\n",
    "        #Kalman filter initialisation\n",
    "        X0 = Noisy_samples[0]\n",
    "        nx0 = 2\n",
    "        X0 = np.zeros((nx0+1,1))\n",
    "        nx0_new = 3\n",
    "\n",
    "        #The margianlised Kalman covariance\n",
    "        C_prior = np.zeros((nx0_new,nx0_new))\n",
    "        C_prior[-1,-1] = kw\n",
    "\n",
    "        g = np.array([[1],[0],[0]])\n",
    "        g = g.T\n",
    "        R = np.array([kv]) #The noise covariance matrix. Same observation noise throughout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        #Particle filter Initialisation    \n",
    "        initial_particles = []\n",
    "        for i in range(num_particles):\n",
    "            initial_particles.append([np.zeros((nx0,1)),np.eye(nx0)])\n",
    "        previous_Xs = []\n",
    "        previous_X_uncertaintys = []\n",
    "        for i in range(num_particles):\n",
    "            previous_X_uncertaintys.append(C_prior)\n",
    "            previous_Xs.append(X0)\n",
    "        # Define initial weights (uniform)\n",
    "        initial_weights = np.ones(num_particles) / num_particles\n",
    "\n",
    "        \n",
    "\n",
    "        # Time step size and sigma for the likelihood function\n",
    "        dt = evaluation_points[1] - evaluation_points[0]\n",
    "        matrix_exp = expm(A*dt)\n",
    "        sigma = sigma_n #The observatin noise\n",
    "\n",
    "\n",
    "\n",
    "        #Containers for the imnference results\n",
    "        inferred_Xs = []\n",
    "        inferred_covs = []\n",
    "        first_time = True\n",
    "\n",
    "        # Running the particle filter\n",
    "        particles = initial_particles\n",
    "        histories = [particles]\n",
    "        weights = initial_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "        sigmaw_values = []\n",
    "        sigmaw_uncertainties = []\n",
    "\n",
    "        for i in range(len(evaluation_points)): #i is the time index we want for N\n",
    "            #sigmaw here needs to be updated in every step\n",
    "            alphaw = weighted_sum(alphaws,weights)\n",
    "            betaw = weighted_sum(betaws,weights)\n",
    "            sigmaw2,sigmaw_uncertainty = inverted_gamma_to_mean_variance(alphaw, betaw) #Note that this is sigmaw^2 but not sigmaw\n",
    "            sigmaw = np.sqrt(sigmaw2)\n",
    "\n",
    "            sigmaw_values.append(sigmaw)\n",
    "            sigmaw_uncertainties.append(sigmaw_uncertainty)\n",
    "\n",
    "\n",
    "            incremental_normal_gamma_generator = normal_gamma_process(beta, C, dt, 0, sigmaw) #We are just using the built in gamma generator inside， putting in some random muw or sigmaw has no effect\n",
    "            incremental_SDE = SDE(A,h,dt,incremental_normal_gamma_generator)\n",
    "            #print(i)\n",
    "            t = evaluation_points[i]\n",
    "            observation = Noisy_samples[i] #Note that the observation here is still a row vector, reshaping necessary\n",
    "            previous_Xs, previous_X_uncertaintys,particles,weights,alphaws, betaws, accumulated_Es, accumulated_Fs,log_marginals  = ultimate_NVM_pf(observation, previous_Xs, previous_X_uncertaintys, particles, transition_function_ultimate_NVM_pf, matrix_exp, dt,incremental_SDE,g,R,alphaws,betaws,accumulated_Es,accumulated_Fs,i,return_log_marginals=True) # N is the time nidex\n",
    "            inferred_cov = weighted_sum(previous_X_uncertaintys,weights) * sigmaw2 #Note that the original parameters are marginalised by sigmaw^2\n",
    "            inferred_X = weighted_sum(previous_Xs,weights)\n",
    "            \n",
    "            histories.append(particles)\n",
    "            \n",
    "            inferred_Xs.append(inferred_X)\n",
    "            inferred_covs.append(inferred_cov)\n",
    "            log_marginals = np.array(log_marginals)\n",
    "\n",
    "        original_state_log_probability = logsumexp(log_marginals) - np.log(num_particles)\n",
    "        first_time = False\n",
    "\n",
    "    #####################################################################################################################################################################################################################################################\n",
    "    #From here, we propose a theta sample\n",
    "\n",
    "    theta_proposed = theta_samples[-1] + np.random.randn() * l\n",
    "\n",
    "    #print(\"Progress:\",progress/searching_resolution, \"%\")\n",
    "    #progress +=1\n",
    "    #Prior inverted gamma parameters for sigmaw\n",
    "    alphaws = 2.1 * np.ones(num_particles)\n",
    "    betaws = 1 * np.ones(num_particles)\n",
    "    accumulated_Es = np.zeros(num_particles)\n",
    "    accumulated_Fs = np.zeros(num_particles)\n",
    "\n",
    "    trajectory = []\n",
    "    A = np.zeros((2, 2))\n",
    "    A[0, 1] = 1\n",
    "    A[1, 1] = theta_proposed\n",
    "\n",
    "    #Kalman filter initialisation\n",
    "    X0 = Noisy_samples[0]\n",
    "    nx0 = 2\n",
    "    X0 = np.zeros((nx0+1,1))\n",
    "    nx0_new = 3\n",
    "\n",
    "    #The margianlised Kalman covariance\n",
    "    C_prior = np.zeros((nx0_new,nx0_new))\n",
    "    C_prior[-1,-1] = kw\n",
    "\n",
    "    g = np.array([[1],[0],[0]])\n",
    "    g = g.T\n",
    "    R = np.array([kv]) #The noise covariance matrix. Same observation noise throughout\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #Particle filter Initialisation    \n",
    "    initial_particles = []\n",
    "    for i in range(num_particles):\n",
    "        initial_particles.append([np.zeros((nx0,1)),np.eye(nx0)])\n",
    "    previous_Xs = []\n",
    "    previous_X_uncertaintys = []\n",
    "    for i in range(num_particles):\n",
    "        previous_X_uncertaintys.append(C_prior)\n",
    "        previous_Xs.append(X0)\n",
    "    # Define initial weights (uniform)\n",
    "    initial_weights = np.ones(num_particles) / num_particles\n",
    "\n",
    "    \n",
    "\n",
    "    # Time step size and sigma for the likelihood function\n",
    "    dt = evaluation_points[1] - evaluation_points[0]\n",
    "    matrix_exp = expm(A*dt)\n",
    "    sigma = sigma_n #The observatin noise\n",
    "\n",
    "\n",
    "\n",
    "    #Containers for the imnference results\n",
    "    inferred_Xs = []\n",
    "    inferred_covs = []\n",
    "    first_time = True\n",
    "\n",
    "    # Running the particle filter\n",
    "    particles = initial_particles\n",
    "    histories = [particles]\n",
    "    weights = initial_weights\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    sigmaw_values = []\n",
    "    sigmaw_uncertainties = []\n",
    "\n",
    "    for i in range(len(evaluation_points)): #i is the time index we want for N\n",
    "        #sigmaw here needs to be updated in every step\n",
    "        alphaw = weighted_sum(alphaws,weights)\n",
    "        betaw = weighted_sum(betaws,weights)\n",
    "        sigmaw2,sigmaw_uncertainty = inverted_gamma_to_mean_variance(alphaw, betaw) #Note that this is sigmaw^2 but not sigmaw\n",
    "        sigmaw = np.sqrt(sigmaw2)\n",
    "\n",
    "        sigmaw_values.append(sigmaw)\n",
    "        sigmaw_uncertainties.append(sigmaw_uncertainty)\n",
    "\n",
    "\n",
    "        incremental_normal_gamma_generator = normal_gamma_process(beta, C, dt, 0, sigmaw) #We are just using the built in gamma generator inside， putting in some random muw or sigmaw has no effect\n",
    "        incremental_SDE = SDE(A,h,dt,incremental_normal_gamma_generator)\n",
    "        #print(i)\n",
    "        t = evaluation_points[i]\n",
    "        observation = Noisy_samples[i] #Note that the observation here is still a row vector, reshaping necessary\n",
    "        previous_Xs, previous_X_uncertaintys,particles,weights,alphaws, betaws, accumulated_Es, accumulated_Fs,log_marginals  = ultimate_NVM_pf(observation, previous_Xs, previous_X_uncertaintys, particles, transition_function_ultimate_NVM_pf, matrix_exp, dt,incremental_SDE,g,R,alphaws,betaws,accumulated_Es,accumulated_Fs,i,return_log_marginals=True) # N is the time nidex\n",
    "        inferred_cov = weighted_sum(previous_X_uncertaintys,weights) * sigmaw2 #Note that the original parameters are marginalised by sigmaw^2\n",
    "        inferred_X = weighted_sum(previous_Xs,weights)\n",
    "        \n",
    "        histories.append(particles)\n",
    "        \n",
    "        inferred_Xs.append(inferred_X)\n",
    "        inferred_covs.append(inferred_cov)\n",
    "        log_marginals = np.array(log_marginals)\n",
    "\n",
    "    proposed_state_log_probability = logsumexp(log_marginals) - np.log(num_particles)\n",
    "\n",
    "\n",
    "    ##################################################################################################################################################################################################################################\n",
    "    # Acceptance Attempt\n",
    "    log_acceptance_ratio = proposed_state_log_probability - original_state_log_probability\n",
    "    if np.log(np.random.rand())< log_acceptance_ratio: #Accepted case\n",
    "        theta_samples.append(theta_proposed)\n",
    "        original_state_log_probability = proposed_state_log_probability\n",
    "    else: #Rejected case\n",
    "        theta_samples.append(theta_samples[-1])\n",
    "\n",
    "\n",
    "\n",
    "theta_samples = np.array(theta_samples)  # 假设已经去除了燃烧期的样本\n",
    "\n",
    "# 后验分布摘要\n",
    "mean_theta = np.mean(theta_samples)\n",
    "median_theta = np.median(theta_samples)\n",
    "conf_interval = np.percentile(theta_samples, [2.5, 97.5])  # 95%置信区间\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(theta_samples)\n",
    "plt.xlabel(\"Iteration Number\")\n",
    "plt.ylabel(\"Theta Value\")\n",
    "plt.title(\"Theta Samples from PMMH\")\n",
    "plt.show()\n",
    "\n",
    "# 可视化后验分布\n",
    "plt.figure(figsize=(10, 6))\n",
    "sns.histplot(theta_samples, kde=True, stat=\"density\", linewidth=0)\n",
    "plt.axvline(mean_theta, color='r', linestyle='--', label=f'Mean: {mean_theta:.2f}')\n",
    "plt.axvline(conf_interval[0], color='g', linestyle=':', label='95% CI')\n",
    "plt.axvline(conf_interval[1], color='g', linestyle=':')\n",
    "plt.title('Posterior distribution of $\\\\theta$')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# 打印摘要统计量\n",
    "print(f\"Mean of theta: {mean_theta}\")\n",
    "print(f\"Median of theta: {median_theta}\")\n",
    "print(f\"95% confidence interval of theta: {conf_interval}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "y4proj_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
